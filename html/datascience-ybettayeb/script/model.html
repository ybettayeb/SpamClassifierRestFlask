<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>datascience-ybettayeb.script.model API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>datascience-ybettayeb.script.model</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python
# coding: utf-8


import pandas as pd
import sys
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
import pickle

def trainAndDumpVectorizer(path=None):
    &#34;&#34;&#34;
    This function is your bread and butter, she will open the dataset you will pass in command line argument or use the default &#34;spam.csv&#34; one
    She will cleanup the data by transforming the class into binary value
    She will train a simple word count model and dump the model and the vectorizer in the current folder
    
    to further train the model you can start coding before line 46 which are the model dumping. 
    

    Args:
        path (str, optional): path to the csv file. Defaults to None.
    
    Returns:
        vectorizer: fitted count Vectorizer
        classifier: trained classifier
    &#34;&#34;&#34;

    if path == None:
        path = &#34;spam.csv&#34; #If no arguments are passed we use the default csv
    df = pd.read_csv(path,delimiter=&#34;,&#34;,error_bad_lines=False,header=None,
    names=[&#34;CLASS&#34;, &#34;CONTENT&#34;])
    df[&#34;CONTENT&#34;] = df[&#34;CONTENT&#34;].astype(str)
    df = df.replace({&#34;CLASS&#34;: {&#34;spam&#34;:1,&#34;not_spam&#34;:0}}) # working with binary looks better

    df_train, df_test = train_test_split(df, test_size=0.2, random_state=57)
    y_train = df_train[&#39;CLASS&#39;].values
    vectorizer = CountVectorizer() # We tokenize our dataset, so count each time a word or token appeared and return an array
    X_train = vectorizer.fit_transform(list(df_train[&#39;CONTENT&#39;].values)).todense()

    classifier = LogisticRegression().fit(X_train,y_train)
    pickle.dump(classifier, open(&#39;final_prediction.pickle&#39;, &#39;wb&#39;))
    pickle.dump(vectorizer, open(&#39;final_prediction_vectorizer.pickle&#39;, &#39;wb&#39;))
    return vectorizer, classifier

def predict(message):
    &#34;&#34;&#34;
    basic prediction function.
    train and loads the model before making a prediction and returning the result
    Args:
        message (str): message that you want to test

    Returns:
        int: result of the prediction, 1 if it&#39;s a spam 0 otherwise
    &#34;&#34;&#34;
    vectorizer,classifier = trainAndDumpVectorizer()
    X_test = vectorizer.transform(message).todense()
    predction = classifier.predict(X_test)
    return predction[0]



if __name__ == &#39;__main__&#39;:

    &#34;&#34;&#34;
    Base usage will be : python3 model.py pathToDataSet MessageToCheck
    &#34;&#34;&#34;
    path = sys.argv[0] # we&#39;ll pass the source file as a CLI argument
    message = sys.argv[1]
    vectorizer,classifier = trainAndDumpVectorizer()
    print(predict(message))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="datascience-ybettayeb.script.model.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>message)</span>
</code></dt>
<dd>
<div class="desc"><p>basic prediction function.
train and loads the model before making a prediction and returning the result</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>message</code></strong> :&ensp;<code>str</code></dt>
<dd>message that you want to test</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>result of the prediction, 1 if it's a spam 0 otherwise</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(message):
    &#34;&#34;&#34;
    basic prediction function.
    train and loads the model before making a prediction and returning the result
    Args:
        message (str): message that you want to test

    Returns:
        int: result of the prediction, 1 if it&#39;s a spam 0 otherwise
    &#34;&#34;&#34;
    vectorizer,classifier = trainAndDumpVectorizer()
    X_test = vectorizer.transform(message).todense()
    predction = classifier.predict(X_test)
    return predction[0]</code></pre>
</details>
</dd>
<dt id="datascience-ybettayeb.script.model.trainAndDumpVectorizer"><code class="name flex">
<span>def <span class="ident">trainAndDumpVectorizer</span></span>(<span>path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>This function is your bread and butter, she will open the dataset you will pass in command line argument or use the default "spam.csv" one
She will cleanup the data by transforming the class into binary value
She will train a simple word count model and dump the model and the vectorizer in the current folder</p>
<p>to further train the model you can start coding before line 46 which are the model dumping. </p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>path to the csv file. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>vectorizer</code></dt>
<dd>fitted count Vectorizer</dd>
<dt><code>classifier</code></dt>
<dd>trained classifier</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def trainAndDumpVectorizer(path=None):
    &#34;&#34;&#34;
    This function is your bread and butter, she will open the dataset you will pass in command line argument or use the default &#34;spam.csv&#34; one
    She will cleanup the data by transforming the class into binary value
    She will train a simple word count model and dump the model and the vectorizer in the current folder
    
    to further train the model you can start coding before line 46 which are the model dumping. 
    

    Args:
        path (str, optional): path to the csv file. Defaults to None.
    
    Returns:
        vectorizer: fitted count Vectorizer
        classifier: trained classifier
    &#34;&#34;&#34;

    if path == None:
        path = &#34;spam.csv&#34; #If no arguments are passed we use the default csv
    df = pd.read_csv(path,delimiter=&#34;,&#34;,error_bad_lines=False,header=None,
    names=[&#34;CLASS&#34;, &#34;CONTENT&#34;])
    df[&#34;CONTENT&#34;] = df[&#34;CONTENT&#34;].astype(str)
    df = df.replace({&#34;CLASS&#34;: {&#34;spam&#34;:1,&#34;not_spam&#34;:0}}) # working with binary looks better

    df_train, df_test = train_test_split(df, test_size=0.2, random_state=57)
    y_train = df_train[&#39;CLASS&#39;].values
    vectorizer = CountVectorizer() # We tokenize our dataset, so count each time a word or token appeared and return an array
    X_train = vectorizer.fit_transform(list(df_train[&#39;CONTENT&#39;].values)).todense()

    classifier = LogisticRegression().fit(X_train,y_train)
    pickle.dump(classifier, open(&#39;final_prediction.pickle&#39;, &#39;wb&#39;))
    pickle.dump(vectorizer, open(&#39;final_prediction_vectorizer.pickle&#39;, &#39;wb&#39;))
    return vectorizer, classifier</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="datascience-ybettayeb.script" href="index.html">datascience-ybettayeb.script</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="datascience-ybettayeb.script.model.predict" href="#datascience-ybettayeb.script.model.predict">predict</a></code></li>
<li><code><a title="datascience-ybettayeb.script.model.trainAndDumpVectorizer" href="#datascience-ybettayeb.script.model.trainAndDumpVectorizer">trainAndDumpVectorizer</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>